{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79976f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d03df6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba4247e253340219ff6eb18dcf2540c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd0f88603354d4da838926a7a4a556a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb652214d0a4f41aecc63aaad2ae49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135138cf9b3044c4905d050f15905c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael.todisco/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages/torchvision/datasets/mnist.py:335: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484782168/work/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n"
     ]
    }
   ],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece2482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fadcc730b50>, <torch.utils.data.dataloader.DataLoader object at 0x7fadcc7306a0>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9d8affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVision(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1,\n",
    "                      out_channels=10,\n",
    "                      kernel_size = 3,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 10,\n",
    "                      out_channels=10,\n",
    "                      kernel_size = 3,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(10*7*7, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, len(train_data.classes))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        # x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3dc8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchVision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef6040fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create sample batch of random numbers with same size as image batch\n",
    "images = torch.randn(size=(32, 1, 64, 64)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c07dba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c78ef1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3248, 0.2961, 0.5884,  ..., 0.3674, 0.7405, 0.2887],\n",
       "         [0.5716, 0.3999, 0.6613,  ..., 0.2170, 0.4192, 0.4345],\n",
       "         [0.3965, 0.5439, 0.5014,  ..., 0.6156, 0.2756, 0.1799],\n",
       "         ...,\n",
       "         [0.5585, 0.4988, 0.6639,  ..., 0.4348, 0.3700, 0.4278],\n",
       "         [0.3271, 0.5307, 0.4942,  ..., 0.5747, 0.4504, 0.2288],\n",
       "         [0.4078, 0.6205, 0.6547,  ..., 0.4013, 0.5744, 0.5313]],\n",
       "\n",
       "        [[0.2417, 0.2717, 0.3061,  ..., 0.3539, 0.1219, 0.0000],\n",
       "         [0.0968, 0.0054, 0.4220,  ..., 0.2191, 0.2372, 0.0958],\n",
       "         [0.3004, 0.1026, 0.3374,  ..., 0.1954, 0.4295, 0.2660],\n",
       "         ...,\n",
       "         [0.1222, 0.1351, 0.7613,  ..., 0.2414, 0.2301, 0.4514],\n",
       "         [0.3146, 0.5164, 0.4512,  ..., 0.1715, 0.0000, 0.1450],\n",
       "         [0.0867, 0.0888, 0.4363,  ..., 0.1313, 0.7915, 0.2709]],\n",
       "\n",
       "        [[0.5177, 0.4038, 0.1598,  ..., 0.4074, 0.0000, 0.4368],\n",
       "         [0.0997, 0.1811, 0.1872,  ..., 0.3580, 0.4082, 0.6276],\n",
       "         [0.5486, 0.7778, 0.3136,  ..., 0.5472, 0.3442, 0.5002],\n",
       "         ...,\n",
       "         [0.0349, 0.3871, 0.2051,  ..., 0.3035, 0.3447, 0.4717],\n",
       "         [0.1109, 0.3122, 0.3190,  ..., 0.4613, 0.2060, 0.4448],\n",
       "         [0.0616, 0.5837, 0.2805,  ..., 0.4431, 0.5167, 0.4398]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4805, 0.6191, 0.2481,  ..., 0.1696, 0.2976, 0.1834],\n",
       "         [0.2404, 0.2355, 0.3685,  ..., 0.4173, 0.3472, 0.3267],\n",
       "         [0.4271, 0.3287, 0.6416,  ..., 0.1691, 0.2316, 0.3122],\n",
       "         ...,\n",
       "         [0.4196, 0.4543, 0.4534,  ..., 0.1036, 0.3099, 0.3540],\n",
       "         [0.2240, 0.2114, 0.0000,  ..., 0.2295, 0.2452, 0.3094],\n",
       "         [0.2240, 0.3192, 0.0430,  ..., 0.0707, 0.1539, 0.2451]],\n",
       "\n",
       "        [[0.4440, 0.5144, 0.6552,  ..., 0.8095, 0.6779, 0.4299],\n",
       "         [0.5112, 0.6037, 0.4755,  ..., 0.7717, 0.9691, 0.8988],\n",
       "         [0.8549, 0.5715, 0.8131,  ..., 0.4713, 0.5588, 0.4582],\n",
       "         ...,\n",
       "         [0.8237, 1.4451, 0.8303,  ..., 0.5197, 0.6782, 0.7060],\n",
       "         [0.6869, 0.9447, 0.7207,  ..., 0.6337, 0.7350, 0.6481],\n",
       "         [0.8854, 0.8220, 0.6524,  ..., 0.5753, 0.7087, 0.5562]],\n",
       "\n",
       "        [[0.1141, 0.0015, 0.1907,  ..., 0.0447, 0.2906, 0.4995],\n",
       "         [0.0463, 0.2676, 0.3450,  ..., 0.5573, 0.2615, 0.4265],\n",
       "         [0.0000, 0.2811, 0.3214,  ..., 0.5406, 0.1360, 0.1011],\n",
       "         ...,\n",
       "         [0.3807, 0.2463, 0.0275,  ..., 0.3678, 0.0000, 0.4606],\n",
       "         [0.6951, 0.1486, 0.0000,  ..., 0.2849, 0.2208, 0.1492],\n",
       "         [0.4485, 0.1838, 0.0950,  ..., 0.5624, 0.0000, 0.0000]]],\n",
       "       grad_fn=<MaxPool2DWithIndicesBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9646bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
